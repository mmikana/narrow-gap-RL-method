import numpy as np
import torch
import os
import time
import matplotlib.pyplot as plt
import yaml
from scipy.ndimage import gaussian_filter
from importlib import import_module  # ç”¨äºåŠ¨æ€å¯¼å…¥ç±»


torch.set_num_threads(1)
# åŠ è½½é…ç½®æ–‡ä»¶
def load_config(config_path):
    # è¯»å–æ–‡ä»¶æ—¶æŒ‡å®š encoding='utf-8'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    # è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³
    config['timestamp'] = time.strftime('%Y%m%d-%H%M%S')
    # åˆ›å»ºç›®å½•
    for dir_key in ['model_dir', 'episode_plot_dir', 'training_plot_dir']:
        os.makedirs(config['paths'][dir_key], exist_ok=True)
    return config


# åŠ¨æ€å¯¼å…¥ç±»ï¼ˆæ”¯æŒä¸åŒç®—æ³•ã€ç¯å¢ƒã€å¯è§†åŒ–å™¨ï¼‰
def dynamic_import(class_path):
    module_name, class_name = class_path.rsplit('.', 1)
    module = import_module(module_name)
    return getattr(module, class_name)


# ä¸»è®­ç»ƒå‡½æ•°
def main(config_path='config.yaml'):
    config = load_config(config_path)

    # åŠ¨æ€åˆ›å»ºç¯å¢ƒ
    env_class = dynamic_import(f"Env.{config['env']['type']}")  # ç¯å¢ƒåœ¨Envæ¨¡å—ä¸­
    env = env_class(**config['env']['parameters'])

    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    config['agent']['parameters']['state_dim'] = state_dim
    config['agent']['parameters']['action_dim'] = action_dim

    print(f"State dimension: {state_dim}")
    print(f"Action dimension: {action_dim}")

    # åŠ¨æ€åˆ›å»ºæ™ºèƒ½ä½“
    agent_class = dynamic_import(f"SAC_agent.{config['agent']['type']}")  # å‡è®¾æ™ºèƒ½ä½“åœ¨SAC_agentæ¨¡å—ä¸­
    agent = agent_class(**config['agent']['parameters'])

    # åˆå§‹åŒ–è·Ÿè¸ªå˜é‡
    reward_buffer = []
    episode_lengths = []
    collision_rates = []
    success_rates = []
    reward_best = -np.inf

    # è®­ç»ƒè¿›åº¦è·Ÿè¸ª
    print("Starting training...")
    print(f"{'Episode':<8} {'Reward':<10} {'Avg Reward':<12} {'Length':<8} {'Success':<8} {'Collision':<10}")

    # è®­ç»ƒå¾ªç¯
    for episode_i in range(config['training']['episode_num']):
        reward_episode = 0
        state, info = env.reset()
        episode_collision = False
        episode_success = False
        steps_per_episodes = 0

        for step_i in range(config['training']['step_num']):
            # è·å–åŠ¨ä½œ
            action = agent.get_action(state, add_noise=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(action)

            # å­˜å‚¨ç»éªŒ
            agent.Replay_Buffer.add_memory(state, action, reward, next_state, terminated)

            # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
            reward_episode += reward
            state = next_state

            # æ›´æ–°agent
            if step_i % 4 == 0:
                agent.update()

            # è®°å½•ç¢°æ’å’ŒæˆåŠŸ
            if info.get("collision", False):
                episode_collision = True
            if info.get("goal_achieved", False):
                episode_success = True

            if terminated or truncated:
                break
            steps_per_episodes += 1

        # è®°å½•æœ¬å›åˆæ•°æ®
        reward_buffer.append(reward_episode)
        episode_lengths.append(steps_per_episodes)
        collision_rates.append(1 if episode_collision else 0)
        success_rates.append(1 if episode_success else 0)

        # è®¡ç®—æ»‘åŠ¨å¹³å‡å¥–åŠ±
        window = min(len(reward_buffer), 10)
        reward_avg = np.mean(reward_buffer[-window:])

        # è®¡ç®—æˆåŠŸç‡
        success_window = min(len(success_rates), 50)
        recent_success_rate = np.mean(success_rates[-success_window:]) * 100 if success_rates else 0

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if config['training']['save_best_model'] and reward_avg > reward_best:
            reward_best = reward_avg
            model_dir = config['paths']['model_dir']
            torch.save(agent.actor.state_dict(), f"{model_dir}quadrotor_actor_best_{config['timestamp']}.pth")
            torch.save(agent.critic_1.state_dict(), f"{model_dir}quadrotor_critic1_best_{config['timestamp']}.pth")
            torch.save(agent.critic_2.state_dict(), f"{model_dir}quadrotor_critic2_best_{config['timestamp']}.pth")
            print(f"âœ“ Saving model with best avg reward: {reward_best:.2f}")

        # å®šæœŸå¯è§†åŒ–
        if (episode_i + 1) % config['training']['visualize_interval'] == 0 and episode_i > 0:
            print(f"ğŸ“Š å¯è§†åŒ–ç¬¬ {episode_i} å›åˆçš„é£è¡Œæ•°æ®...")
            episode_data = env.get_episode_data()

            # åŠ¨æ€åˆ›å»ºå¯è§†åŒ–å™¨
            visualizer_class = dynamic_import(f"episode_visualizer.{config['visualizer']['type']}")
            visualizer = visualizer_class(**config['visualizer']['parameters'])

            # å¯è§†åŒ–å‚æ•°
            step_interval = max(1, len(episode_data['trajectory']) // 20)
            visualizer.visualize_episode(
                trajectory=episode_data['trajectory'],
                orientations=episode_data['orientations'],
                velocities=episode_data['velocities'],
                narrow_gap=env.NarrowGap,
                goal_position=env.goal_position,
                step_interval=step_interval
            )

            # ä¿å­˜å¯è§†åŒ–ç»“æœ
            if config['training']['save_episode_plots']:
                plot_dir = config['paths']['episode_plot_dir']
                visualizer.save_plot(f"{plot_dir}episode_{episode_i}_{config['timestamp']}.png")

            plt.show()
            plt.close('all')

        # æ‰“å°è®­ç»ƒè¿›åº¦
        print(
            f'{episode_i:<8} {reward_episode:<10.2f} {reward_avg:<12.2f} {steps_per_episodes:<8} {episode_success:<8}'
            f' {episode_collision:<10}')

        # è¯¾ç¨‹å­¦ä¹ ï¼ˆå¢åŠ éš¾åº¦ï¼‰
        if recent_success_rate > 70 and hasattr(env, 'increase_difficulty'):
            env.increase_difficulty()
            print(f"ğŸ¯ Increasing difficulty! Current level: {env.current_difficulty}")

    env.close()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model_dir = config['paths']['model_dir']
    torch.save(agent.actor.state_dict(), f"{model_dir}quadrotor_actor_final_{config['timestamp']}.pth")
    torch.save(agent.critic_1.state_dict(), f"{model_dir}quadrotor_critic1_final_{config['timestamp']}.pth")
    torch.save(agent.critic_2.state_dict(), f"{model_dir}quadrotor_critic2_final_{config['timestamp']}.pth")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if config['training']['plot_reward']:
        plt.figure(figsize=(15, 10))

        # å¥–åŠ±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(reward_buffer, color='purple', alpha=0.3, label='Episode Reward')
        smoothed_reward = gaussian_filter(reward_buffer, sigma=5)
        plt.plot(smoothed_reward, color='purple', linewidth=2, label='Smoothed Reward')
        plt.title('Training Reward')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.legend()
        plt.grid(True)

        # å›åˆé•¿åº¦æ›²çº¿
        plt.subplot(2, 2, 2)
        plt.plot(episode_lengths, color='blue', alpha=0.3, label='Episode Length')
        smoothed_length = gaussian_filter(episode_lengths, sigma=5)
        plt.plot(smoothed_length, color='blue', linewidth=2, label='Smoothed Length')
        plt.title('Episode Length')
        plt.xlabel('Episode')
        plt.ylabel('Steps')
        plt.legend()
        plt.grid(True)

        # æˆåŠŸç‡æ›²çº¿
        plt.subplot(2, 2, 3)
        window_size = config['training']['window_size']
        success_rates_smoothed = []
        for i in range(len(success_rates)):
            start = max(0, i - window_size + 1)
            success_rates_smoothed.append(np.mean(success_rates[start:i + 1]) * 100)
        plt.plot(success_rates_smoothed, color='green', linewidth=2)
        plt.title(f'Success Rate (Last {window_size} episodes)')
        plt.xlabel('Episode')
        plt.ylabel('Success Rate (%)')
        plt.ylim(0, 100)
        plt.grid(True)

        # ç¢°æ’ç‡æ›²çº¿
        plt.subplot(2, 2, 4)
        collision_rates_smoothed = []
        for i in range(len(collision_rates)):
            start = max(0, i - window_size + 1)
            collision_rates_smoothed.append(np.mean(collision_rates[start:i + 1]) * 100)
        plt.plot(collision_rates_smoothed, color='red', linewidth=2)
        plt.title(f'Collision Rate (Last {window_size} episodes)')
        plt.xlabel('Episode')
        plt.ylabel('Collision Rate (%)')
        plt.ylim(0, 100)
        plt.grid(True)

        plt.tight_layout()
        training_plots = config['paths']['training_plot_dir']
        plt.savefig(f"{training_plots}Training_Results_{config['timestamp']}.png", dpi=300, bbox_inches='tight')
        plt.show()

    print("Training completed!")
    print(f"Best average reward: {reward_best:.2f}")
    print(f"Final success rate: {np.mean(success_rates[-20:]) * 100:.1f}%")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file')
    args = parser.parse_args()
    main(args.config)
