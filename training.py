import numpy as np
import torch
import os
import time
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter

from episode_visualizer import EpisodeVisualizer
from ENV import QuadrotorEnv
from SAC_agent import SACAgent


# The directory to save model
current_path = os.path.dirname(os.path.realpath(__file__))
model_dir = current_path + '/models/'
os.makedirs(model_dir, exist_ok=True)  # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
timestamp = time.strftime('%Y%m%d-%H%M%S')

# Create environment
env = QuadrotorEnv()
STATE_DIM = env.observation_space.shape[0]  # åº”è¯¥æ˜¯12
ACTION_DIM = env.action_space.shape[0]  # åº”è¯¥æ˜¯4


print(f"State dimension: {STATE_DIM}")
print(f"Action dimension: {ACTION_DIM}")

# Set parameters
EPISODE_NUM = 10
STEP_NUM = 1000
VISUALIZE_INTERVAL = 10
SAVE_EPISODE_PLOTS = True

# Choose Agent111
agent = SACAgent(state_dim=STATE_DIM, action_dim=ACTION_DIM, memo_capacity=10000, lr_actor=3e-4, lr_critic=3e-4,
                 gamma=0.99, tau=0.005, layer1_dim=256, layer2_dim=256, batch_size=256)

# Initialization
reward_buffer = []
episode_lengths = []
collision_rates = []
success_rates = []
reward_best = -np.inf  # åˆå§‹æœ€ä½³å¥–åŠ±
PLOT_REWARD = True

# Training progress tracking
print("Starting training...")
print(f"{'Episode':<8} {'Reward':<10} {'Avg Reward':<12} {'Length':<8} {'Success':<8} {'Collision':<10}")

# training
for episode_i in range(EPISODE_NUM):
    reward_episode = 0
    state, info = env.reset()
    episode_collision = False
    episode_success = False
    steps_per_episodes = 0

    for step_i in range(STEP_NUM):
        # è·å–åŠ¨ä½œï¼ˆæ·»åŠ æ¢ç´¢å™ªå£°ï¼‰
        action = agent.get_action(state, add_noise=True)

        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, info = env.step(action)

        # å­˜å‚¨ç»éªŒ
        agent.Replay_Buffer.add_memory(state, action, reward, next_state, terminated)

        # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
        reward_episode += reward
        state = next_state

        # æ›´æ–°agentï¼ˆå®šæœŸæ›´æ–°ï¼‰
        if step_i % 4 == 0:  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡ï¼Œæé«˜æ•°æ®æ•ˆç‡
            agent.update()

        # è®°å½•ç¢°æ’å’ŒæˆåŠŸ
        if info.get("collision", False):
            episode_collision = True
        if info.get("goal_achieved", False):
            episode_success = True

        if terminated or truncated:
            break
        steps_per_episodes += 1

    # è®°å½•æœ¬å›åˆæ•°æ®
    reward_buffer.append(reward_episode)
    episode_lengths.append(steps_per_episodes)
    collision_rates.append(1 if episode_collision else 0)
    success_rates.append(1 if episode_success else 0)

    # è®¡ç®—æ»‘åŠ¨å¹³å‡å¥–åŠ±
    if len(reward_buffer) > 10:
        reward_avg = np.mean(reward_buffer[-10:])  # æœ€è¿‘10ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±
    else:
        reward_avg = np.mean(reward_buffer)

    # è®¡ç®—æˆåŠŸç‡ï¼ˆæœ€è¿‘50ä¸ªå›åˆï¼‰
    if len(success_rates) > 50:
        recent_success_rate = np.mean(success_rates[-50:]) * 100
    else:
        recent_success_rate = np.mean(success_rates) * 100 if success_rates else 0

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if reward_avg > reward_best:
        reward_best = reward_avg
        torch.save(agent.actor.state_dict(), model_dir + f'quadrotor_actor_best_{timestamp}.pth')
        torch.save(agent.critic_1.state_dict(), model_dir + f'quadrotor_critic1_best_{timestamp}.pth')
        torch.save(agent.critic_2.state_dict(), model_dir + f'quadrotor_critic2_best_{timestamp}.pth')
        print(f"âœ“ Saving model with best avg reward: {reward_best:.2f}")

    # å®šæœŸå¯è§†åŒ–episode
    if (episode_i + 1) % VISUALIZE_INTERVAL == 0 and episode_i > 0:
        print(f"ğŸ“Š å¯è§†åŒ–ç¬¬ {episode_i} å›åˆçš„é£è¡Œæ•°æ®...")

        # è·å–å½“å‰episodeæ•°æ®
        episode_data = env.get_episode_data()

        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = EpisodeVisualizer()

        # ç”Ÿæˆè¯¦ç»†åˆ†æå›¾
        visualizer.visualize_episode(
            trajectory=episode_data['trajectory'],
            orientations=episode_data['orientations'],
            velocities=episode_data['velocities'],
            narrow_gap=env.NarrowGap,
            goal_position=env.goal_position,
            step_interval=max(1, len(episode_data['trajectory']) // 20)  # åŠ¨æ€è°ƒæ•´ç®­å¤´å¯†åº¦
        )

        # ä¿å­˜åˆ†æå›¾
        if SAVE_EPISODE_PLOTS:
            plot_dir = current_path + '/episode_plots/'
            os.makedirs(plot_dir, exist_ok=True)
            visualizer.save_plot(f"{plot_dir}episode_{episode_i}_{timestamp}.png")

        plt.show()
        plt.close('all')  # å…³é—­æ‰€æœ‰å›¾è¡¨é¿å…å†…å­˜æ³„æ¼

    # æ‰“å°è®­ç»ƒè¿›åº¦
    print(
        f'{episode_i:<8} {reward_episode:<10.2f} {reward_avg:<12.2f} {steps_per_episodes:<8} {episode_success:<8}'
        f' {episode_collision:<10}')

    # å¦‚æœè¿ç»­æˆåŠŸï¼Œå¢åŠ éš¾åº¦ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
    if recent_success_rate > 70 and hasattr(env, 'increase_difficulty'):
        env.increase_difficulty()
        print(f"ğŸ¯ Increasing difficulty! Current level: {env.current_difficulty}")

env.close()


# ä¿å­˜æœ€ç»ˆæ¨¡å‹
torch.save(agent.actor.state_dict(), model_dir + f'quadrotor_actor_final_{timestamp}.pth')
torch.save(agent.critic_1.state_dict(), model_dir + f'quadrotor_critic1_final_{timestamp}.pth')
torch.save(agent.critic_2.state_dict(), model_dir + f'quadrotor_critic2_final_{timestamp}.pth')

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
if PLOT_REWARD:
    plt.figure(figsize=(15, 10))

    # å¥–åŠ±æ›²çº¿
    plt.subplot(2, 2, 1)
    plt.plot(reward_buffer, color='purple', alpha=0.3, label='Episode Reward')
    smoothed_reward = gaussian_filter(reward_buffer, sigma=5)
    plt.plot(smoothed_reward, color='purple', linewidth=2, label='Smoothed Reward')
    plt.title('Training Reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.legend()
    plt.grid(True)

    # å›åˆé•¿åº¦æ›²çº¿
    plt.subplot(2, 2, 2)
    plt.plot(episode_lengths, color='blue', alpha=0.3, label='Episode Length')
    smoothed_length = gaussian_filter(episode_lengths, sigma=5)
    plt.plot(smoothed_length, color='blue', linewidth=2, label='Smoothed Length')
    plt.title('Episode Length')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    plt.legend()
    plt.grid(True)

    # æˆåŠŸç‡æ›²çº¿ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    plt.subplot(2, 2, 3)
    window_size = 20
    success_rates_smoothed = []
    for i in range(len(success_rates)):
        start = max(0, i - window_size + 1)
        success_rates_smoothed.append(np.mean(success_rates[start:i + 1]) * 100)

    plt.plot(success_rates_smoothed, color='green', linewidth=2)
    plt.title(f'Success Rate (Last {window_size} episodes)')
    plt.xlabel('Episode')
    plt.ylabel('Success Rate (%)')
    plt.ylim(0, 100)
    plt.grid(True)

    # ç¢°æ’ç‡æ›²çº¿ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    plt.subplot(2, 2, 4)
    collision_rates_smoothed = []
    for i in range(len(collision_rates)):
        start = max(0, i - window_size + 1)
        collision_rates_smoothed.append(np.mean(collision_rates[start:i + 1]) * 100)

    plt.plot(collision_rates_smoothed, color='red', linewidth=2)
    plt.title(f'Collision Rate (Last {window_size} episodes)')
    plt.xlabel('Episode')
    plt.ylabel('Collision Rate (%)')
    plt.ylim(0, 100)
    plt.grid(True)

    plt.tight_layout()
    training_plots = current_path +'/training_plots/'
    plt.savefig(f"{training_plots}Training_Results_{timestamp}.png", dpi=300, bbox_inches='tight')
    plt.show()

print("Training completed!")
print(f"Best average reward: {reward_best:.2f}")
print(f"Final success rate: {np.mean(success_rates[-20:]) * 100:.1f}%")
